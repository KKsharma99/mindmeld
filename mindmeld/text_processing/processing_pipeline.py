# -*- coding: utf-8 -*-
#
# Copyright (c) 2015 Cisco Systems, Inc. and others.  All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This module contains a text Processing Pipeline."""
import logging
from typing import List
from .preprocessor import Preprocessor, PreprocessorFactory
from .normalizers import Normalizer, NormalizerFactory
from .tokenizers import Tokenizer, TokenizerFactory
from .stemmers import Stemmer, StemmerFactory

from ..components._config import get_language_config

logger = logging.getLogger(__name__)


class ProcessingPipelineError(Exception):
    pass


class ProcessingPipeline:
    """Pipeline Class for MindMeld's text processing."""

    _instance = None

    def __init__(
        self,
        language: str,
        preprocessors: List[Preprocessor],
        normalizers: List[Normalizer],
        tokenizer: Tokenizer,
        stemmer: Stemmer,
    ):
        """Creates a Pipeline instance."""
        self.check_instance_exists()
        self.language = language
        self.preprocessors = preprocessors
        self.normalizers = normalizers
        self.tokenizer = tokenizer
        self.stemmer = stemmer
        ProcessingPipeline._instance = self

    def check_instance_exists(self):
        """ Check if an instance of the ProcessingPipeline singleton exists."""
        if self._instance:
            raise ProcessingPipelineError("ProcessingPipeline is a singleton.")

    # TODO: Add validation functions, ensure instance types are correct

    def preprocess(self, text):
        """
        Args:
            text (str)

        Returns:
            (str)
        """
        for preprocessor in self.preprocessors:
            text = preprocessor.process(text)
        return text

    def normalize(self, text):
        """
        Args:
            text (str): Input text.
        Returns:
            normalized_text (str): Normalized Text.
        """
        for normalizer in self.normalizers:
            text = normalizer.normalize(text)
        return text

    def tokenize(self, text):
        """
        Args:
            text (str): Input text.
        Returns:
            tokens (List[str]): List of tokens.
        """
        return self.tokenizer.tokenize(text)

    def stem_words(self, words):
        """
        Gets the stem of a word. For example, the stem of the word 'fishing' is 'fish'.

        Args:
            words (List[str]): List of words to stem.

        Returns:
            stemmed_words (List[str]): List of stemmed words.
        """
        return [self.stemmer.stem_word(word) for word in words]

    def get_char_index_map(self, raw_text, normalized_text):
        """
        Generates character index mapping from normalized query to raw query. The entity model
        always operates on normalized query during NLP processing but for entity output we need
        to generate indexes based on raw query.

        The mapping is generated by calculating edit distance and backtracking to get the
        proper alignment.

        Args:
            raw_text (str): Raw query text.
            normalized_text (str): Normalized query text.
        Returns:
            dict: A mapping of character indexes from normalized query to raw query.
        """
        text = raw_text.lower()
        # TODO: Check this text = self.fold_str_to_ascii(text)

        m = len(raw_text)
        n = len(normalized_text)

        # handle case where normalized text is the empty string
        if n == 0:
            raw_to_norm_mapping = {i: 0 for i in range(m)}
            return raw_to_norm_mapping, {0: 0}

        # handle case where normalized text and raw text are identical
        if m == n and raw_text == normalized_text:
            mapping = {i: i for i in range(n)}
            return mapping, mapping

        edit_dis = []
        for i in range(0, n + 1):
            edit_dis.append([0] * (m + 1))
        edit_dis[0] = list(range(0, m + 1))
        for i in range(0, n + 1):
            edit_dis[i][0] = i

        directions = []
        for i in range(0, n + 1):
            directions.append([""] * (m + 1))

        for i in range(1, n + 1):
            for j in range(1, m + 1):
                dis = 999
                direction = None

                diag_dis = edit_dis[i - 1][j - 1]
                if normalized_text[i - 1] != text[j - 1]:
                    diag_dis += 1

                # dis from going down
                down_dis = edit_dis[i - 1][j] + 1

                # dis from going right
                right_dis = edit_dis[i][j - 1] + 1

                if down_dis < dis:
                    dis = down_dis
                    direction = "↓"
                if right_dis < dis:
                    dis = right_dis
                    direction = "→"
                if diag_dis < dis:
                    dis = diag_dis
                    direction = "↘"

                edit_dis[i][j] = dis
                directions[i][j] = direction

        mapping = {}

        # backtrack
        m_idx = m
        n_idx = n
        while m_idx > 0 and n_idx > 0:
            if directions[n_idx][m_idx] == "↘":
                mapping[n_idx - 1] = m_idx - 1
                m_idx -= 1
                n_idx -= 1
            elif directions[n_idx][m_idx] == "→":
                m_idx -= 1
            elif directions[n_idx][m_idx] == "↓":
                n_idx -= 1

        # initialize the forward mapping (raw to normalized text)
        raw_to_norm_mapping = {0: 0}

        # naive approach for generating forward mapping. this is naive and probably not robust.
        # all leading special characters will get mapped to index position 0 in normalized text.
        raw_to_norm_mapping.update({v: k for k, v in mapping.items()})
        for i in range(0, m):
            if i not in raw_to_norm_mapping:
                raw_to_norm_mapping[i] = raw_to_norm_mapping[i - 1]

        return raw_to_norm_mapping, mapping


class ProcessorPipelineFactory:
    """Creates a ProcessorPipeline object.
    """

    @staticmethod
    def create_processor_pipeline(app_path):
        """Static method to create a ProcessoPipeline instance.

        Returns:
            ProcessorPipeline: An ProcessorPipeline class.
        """
        # TODO: Check if an instance already exists and return that since it is a singleton
  
        if ProcessingPipeline._instance:
            logger.warning("ProcessingPipeline is a singleton. Instance already "
            "created, returning existing instance.")
            return ProcessingPipeline._instance

        language, _ = get_language_config(app_path)
        config = get_processing_pipeline_config()

        preprocessors = config.get(
            "preprocessors", DEFAULT_PROCESSOR_PIPELINE_CONFIG["preprocessors"]
        )
        preprocessors = [
            PreprocessorFactory.get_preprocessor(p) for p in preprocessors
        ]

        normalizers = config.get(
            "normalizers", DEFAULT_PROCESSOR_PIPELINE_CONFIG["normalizers"]
        )
        normalizers = [
            NormalizerFactory.get_normalizer(n) for n in normalizers
        ]

        tokenizer = config.get(
            "tokenizer", DEFAULT_PROCESSOR_PIPELINE_CONFIG["tokenizer"]
        )

        tokenizer = TokenizerFactory.get_tokenizer(tokenizer)
        stemmer = config.get("stemmer")

        if stemmer:
            stemmer = StemmerFactory.get_stemmer(stemmer)
        else:
            stemmer = StemmerFactory.get_stemmer_by_language(language)

        return ProcessingPipeline(
            language, preprocessors, normalizers, tokenizer, stemmer
        )

# PLACEHOLDER CODE BELOW

PROCESSOR_PIPELINE_CONFIG = {
        "preprocessors": [
            "NoOpPreprocessor"
        ],
        "normalizers": [
            "RegexNormalizer",
            "AsciiFoldNormalizer"
        ],
        "tokenizer": "SpacyTokenizer",
        "stemmer": "EnglishNLTKStemmer"
}

def get_processing_pipeline_config():
    return {
        "preprocessors": [
            "NoOpPreprocessor"
        ],
        "normalizers": [
            "RegexNormalizer",
            "UnicodeCharacterNormalizer"
        ],
        "tokenizer": "SpacyTokenizer",
        "stemmer": "EnglishNLTKStemmer"
    }

# @staticmethod
# def get_instance():
#     if not ProcessingPipeline._instance:
#         ProcessingPipeline()
#     return ProcessingPipeline._instance
